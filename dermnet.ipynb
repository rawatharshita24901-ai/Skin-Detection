{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "15d35a71",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models, regularizers\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import cv2\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.utils import class_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "e5ab1192",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_dir = r\"C:\\Users\\rrawa\\skin\\Derm\\train\"\n",
    "test_dir = r\"C:\\Users\\rrawa\\skin\\Derm\\test\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "268cb104",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classes found: ['Blackheads', 'Cyst', 'Papules', 'Pustules', 'Whiteheads']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "label_names = sorted(os.listdir(train_dir))\n",
    "label_map = {label: index for index, label in enumerate(label_names)}\n",
    "print(\"Classes found:\", label_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "cd26a0fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def load_data(data_dir, img_size=(128, 128)):\n",
    "    images = []\n",
    "    labels = []\n",
    "\n",
    "    for label in os.listdir(data_dir):\n",
    "        label_dir = os.path.join(data_dir, label)\n",
    "        if os.path.isdir(label_dir):\n",
    "            for img_name in os.listdir(label_dir):\n",
    "                img_path = os.path.join(label_dir, img_name)\n",
    "                img = cv2.imread(img_path)  # Read in color (BGR)\n",
    "                if img is not None:\n",
    "                    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)  # Convert to RGB\n",
    "                    img = cv2.resize(img, img_size)\n",
    "                    images.append(img)\n",
    "                    if label in label_map:\n",
    "                        labels.append(label_map[label])\n",
    "                else:\n",
    "                    print(f\"Warning: Could not load {img_path}\")\n",
    "\n",
    "    images = np.array(images, dtype=\"float32\") / 255.0\n",
    "    labels = to_categorical(np.array(labels), num_classes=len(label_names))\n",
    "    return images, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88d627ae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "c663ccec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class weights calculated successfully.\n"
     ]
    }
   ],
   "source": [
    "# Compute class weights for imbalanced dataset\n",
    "# Convert one-hot encoded labels back to class indices\n",
    "train_label_indices = np.argmax(train_labels, axis=1)\n",
    "class_weights = class_weight.compute_class_weight(\n",
    "    class_weight='balanced',\n",
    "    classes=np.unique(train_label_indices),\n",
    "    y=train_label_indices\n",
    ")\n",
    "class_weights = dict(enumerate(class_weights))\n",
    "print(\"Class weights calculated successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "b11d6285",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dir exists: True\n",
      "Subfolders in train_dir: ['Blackheads', 'Cyst', 'Papules', 'Pustules', 'Whiteheads']\n"
     ]
    }
   ],
   "source": [
    "print(\"Train dir exists:\", os.path.exists(train_dir))\n",
    "print(\"Subfolders in train_dir:\", os.listdir(train_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "cae59cd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2778 images belonging to 5 classes.\n",
      "Found 0 images belonging to 5 classes.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Load only test data (smaller dataset)\n",
    "test_images, test_labels = load_data(test_dir)\n",
    "\n",
    "# For training, we'll use ImageDataGenerator with flow_from_directory\n",
    "# to avoid loading all images into memory at once\n",
    "# First, create the training data generator\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rotation_range=25,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    shear_range=0.15,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    fill_mode=\"nearest\",\n",
    "    rescale=1./255\n",
    ")\n",
    "\n",
    "val_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "# Load generators from directory\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    train_dir,\n",
    "    target_size=(128, 128),\n",
    "    batch_size=32,\n",
    "    class_mode='categorical',\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "val_generator = val_datagen.flow_from_directory(\n",
    "    train_dir,\n",
    "    target_size=(128, 128),\n",
    "    batch_size=32,\n",
    "    class_mode='categorical',\n",
    "    shuffle=False,\n",
    "    subset='validation'  # This will need directory structure adjustment\n",
    ")\n",
    "\n",
    "# Alternative: Create validation split from training directory\n",
    "# For now, we'll use a smaller validation set loaded into memory\n",
    "train_images_small, train_labels_small = load_data(train_dir)\n",
    "train_images, val_images, train_labels, val_labels = train_test_split(\n",
    "    train_images_small, train_labels_small, test_size=0.2, random_state=42\n",
    ")\n",
    "del train_images_small, train_labels_small  # Free memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "4fb8e1e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_images, val_images, train_labels, val_labels = train_test_split(\n",
    "    train_images, train_labels, test_size=0.2, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "f3e7dc6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "datagen = ImageDataGenerator(\n",
    "    rotation_range=25,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    shear_range=0.15,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    fill_mode=\"nearest\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "acb5b18e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model = models.Sequential([\n",
    "    layers.Input(shape=(128, 128, 3)),\n",
    "\n",
    "    layers.Conv2D(32, (3, 3), activation=\"relu\", kernel_regularizer=regularizers.l2(0.001)),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.MaxPooling2D(2, 2),\n",
    "\n",
    "    layers.Conv2D(64, (3, 3), activation=\"relu\", kernel_regularizer=regularizers.l2(0.001)),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.MaxPooling2D(2, 2),\n",
    "\n",
    "    layers.Conv2D(128, (3, 3), activation=\"relu\", kernel_regularizer=regularizers.l2(0.001)),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.MaxPooling2D(2, 2),\n",
    "\n",
    "    layers.Conv2D(256, (3, 3), activation=\"relu\", kernel_regularizer=regularizers.l2(0.001)),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.MaxPooling2D(2, 2),\n",
    "\n",
    "    layers.Flatten(),\n",
    "    layers.Dropout(0.4),\n",
    "    layers.Dense(512, activation=\"relu\", kernel_regularizer=regularizers.l2(0.001)),\n",
    "    layers.Dropout(0.5),\n",
    "    layers.Dense(len(label_names), activation=\"softmax\")\n",
    "])\n",
    "\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4),\n",
    "              loss=\"categorical_crossentropy\",\n",
    "              metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "a9633f91",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "early_stopping = EarlyStopping(monitor=\"val_loss\", patience=10, restore_best_weights=True)\n",
    "lr_reduction = ReduceLROnPlateau(monitor=\"val_loss\", patience=3, factor=0.5, min_lr=1e-7, verbose=1)\n",
    "model_checkpoint = ModelCheckpoint(\"best_dermnet_model.keras\", monitor=\"val_loss\", save_best_only=True, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "681ce68c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\rrawa\\Downloads\\PBL\\vml\\Lib\\site-packages\\keras\\src\\trainers\\data_adapters\\py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "\u001b[1m56/56\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 896ms/step - accuracy: 0.2471 - loss: 5.6591\n",
      "Epoch 1: val_loss improved from None to 3.14002, saving model to best_dermnet_model.keras\n",
      "\u001b[1m56/56\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m68s\u001b[0m 1s/step - accuracy: 0.2774 - loss: 5.0949 - val_accuracy: 0.2517 - val_loss: 3.1400 - learning_rate: 1.0000e-04\n",
      "Epoch 2/25\n",
      "\u001b[1m56/56\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 712ms/step - accuracy: 0.3350 - loss: 4.0720\n",
      "Epoch 2: val_loss did not improve from 3.14002\n",
      "\u001b[1m56/56\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 746ms/step - accuracy: 0.3247 - loss: 3.9834 - val_accuracy: 0.1888 - val_loss: 3.8821 - learning_rate: 1.0000e-04\n",
      "Epoch 3/25\n",
      "\u001b[1m56/56\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 697ms/step - accuracy: 0.3103 - loss: 3.6493\n",
      "Epoch 3: val_loss did not improve from 3.14002\n",
      "\u001b[1m56/56\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 727ms/step - accuracy: 0.3286 - loss: 3.5068 - val_accuracy: 0.1865 - val_loss: 4.3186 - learning_rate: 1.0000e-04\n",
      "Epoch 4/25\n",
      "\u001b[1m56/56\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 710ms/step - accuracy: 0.3304 - loss: 3.3199\n",
      "Epoch 4: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-05.\n",
      "\n",
      "Epoch 4: val_loss did not improve from 3.14002\n",
      "\u001b[1m56/56\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 752ms/step - accuracy: 0.3309 - loss: 3.3244 - val_accuracy: 0.2045 - val_loss: 4.7605 - learning_rate: 1.0000e-04\n",
      "Epoch 5/25\n",
      "\u001b[1m56/56\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 712ms/step - accuracy: 0.3433 - loss: 3.1129\n",
      "Epoch 5: val_loss did not improve from 3.14002\n",
      "\u001b[1m56/56\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 743ms/step - accuracy: 0.3472 - loss: 3.1209 - val_accuracy: 0.2000 - val_loss: 4.3362 - learning_rate: 5.0000e-05\n",
      "Epoch 6/25\n",
      "\u001b[1m56/56\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 722ms/step - accuracy: 0.3374 - loss: 3.1029\n",
      "Epoch 6: val_loss did not improve from 3.14002\n",
      "\u001b[1m56/56\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 756ms/step - accuracy: 0.3478 - loss: 3.1185 - val_accuracy: 0.2022 - val_loss: 4.1690 - learning_rate: 5.0000e-05\n",
      "Epoch 7/25\n",
      "\u001b[1m56/56\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 719ms/step - accuracy: 0.3783 - loss: 2.9096\n",
      "Epoch 7: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-05.\n",
      "\n",
      "Epoch 7: val_loss did not improve from 3.14002\n",
      "\u001b[1m56/56\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 752ms/step - accuracy: 0.3618 - loss: 2.9604 - val_accuracy: 0.2180 - val_loss: 3.9669 - learning_rate: 5.0000e-05\n",
      "Epoch 8/25\n",
      "\u001b[1m56/56\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 718ms/step - accuracy: 0.3475 - loss: 2.9721\n",
      "Epoch 8: val_loss did not improve from 3.14002\n",
      "\u001b[1m56/56\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 751ms/step - accuracy: 0.3517 - loss: 2.9613 - val_accuracy: 0.2225 - val_loss: 3.4640 - learning_rate: 2.5000e-05\n",
      "Epoch 9/25\n",
      "\u001b[1m56/56\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 705ms/step - accuracy: 0.3818 - loss: 2.9105\n",
      "Epoch 9: val_loss did not improve from 3.14002\n",
      "\u001b[1m56/56\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 737ms/step - accuracy: 0.3748 - loss: 2.9041 - val_accuracy: 0.2360 - val_loss: 3.1859 - learning_rate: 2.5000e-05\n",
      "Epoch 10/25\n",
      "\u001b[1m56/56\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 712ms/step - accuracy: 0.3663 - loss: 2.8211\n",
      "Epoch 10: val_loss improved from 3.14002 to 2.97505, saving model to best_dermnet_model.keras\n",
      "\u001b[1m56/56\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 773ms/step - accuracy: 0.3545 - loss: 2.8571 - val_accuracy: 0.2629 - val_loss: 2.9750 - learning_rate: 2.5000e-05\n",
      "Epoch 11/25\n",
      "\u001b[1m56/56\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 704ms/step - accuracy: 0.3896 - loss: 2.9261\n",
      "Epoch 11: val_loss improved from 2.97505 to 2.80882, saving model to best_dermnet_model.keras\n",
      "\u001b[1m56/56\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 742ms/step - accuracy: 0.3827 - loss: 2.8833 - val_accuracy: 0.2921 - val_loss: 2.8088 - learning_rate: 2.5000e-05\n",
      "Epoch 12/25\n",
      "\u001b[1m56/56\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 705ms/step - accuracy: 0.3724 - loss: 2.8066\n",
      "Epoch 12: val_loss improved from 2.80882 to 2.68681, saving model to best_dermnet_model.keras\n",
      "\u001b[1m56/56\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 747ms/step - accuracy: 0.3827 - loss: 2.8173 - val_accuracy: 0.3618 - val_loss: 2.6868 - learning_rate: 2.5000e-05\n",
      "Epoch 13/25\n",
      "\u001b[1m56/56\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 709ms/step - accuracy: 0.3813 - loss: 2.8138\n",
      "Epoch 13: val_loss improved from 2.68681 to 2.63701, saving model to best_dermnet_model.keras\n",
      "\u001b[1m56/56\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 747ms/step - accuracy: 0.3748 - loss: 2.8361 - val_accuracy: 0.3573 - val_loss: 2.6370 - learning_rate: 2.5000e-05\n",
      "Epoch 14/25\n",
      "\u001b[1m56/56\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 721ms/step - accuracy: 0.3623 - loss: 2.8698\n",
      "Epoch 14: val_loss improved from 2.63701 to 2.62860, saving model to best_dermnet_model.keras\n",
      "\u001b[1m56/56\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 761ms/step - accuracy: 0.3799 - loss: 2.7800 - val_accuracy: 0.3708 - val_loss: 2.6286 - learning_rate: 2.5000e-05\n",
      "Epoch 15/25\n",
      "\u001b[1m56/56\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 674ms/step - accuracy: 0.3941 - loss: 2.7850\n",
      "Epoch 15: val_loss improved from 2.62860 to 2.58200, saving model to best_dermnet_model.keras\n",
      "\u001b[1m56/56\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m79s\u001b[0m 710ms/step - accuracy: 0.3939 - loss: 2.7726 - val_accuracy: 0.3865 - val_loss: 2.5820 - learning_rate: 2.5000e-05\n",
      "Epoch 16/25\n",
      "\u001b[1m56/56\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 702ms/step - accuracy: 0.3831 - loss: 2.7823\n",
      "Epoch 16: val_loss did not improve from 2.58200\n",
      "\u001b[1m56/56\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 734ms/step - accuracy: 0.3844 - loss: 2.7732 - val_accuracy: 0.4000 - val_loss: 2.6003 - learning_rate: 2.5000e-05\n",
      "Epoch 17/25\n",
      "\u001b[1m56/56\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 702ms/step - accuracy: 0.3685 - loss: 2.7964\n",
      "Epoch 17: val_loss did not improve from 2.58200\n",
      "\u001b[1m56/56\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 734ms/step - accuracy: 0.3832 - loss: 2.7796 - val_accuracy: 0.4045 - val_loss: 2.5887 - learning_rate: 2.5000e-05\n",
      "Epoch 18/25\n",
      "\u001b[1m56/56\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 701ms/step - accuracy: 0.3929 - loss: 2.7163\n",
      "Epoch 18: val_loss improved from 2.58200 to 2.56948, saving model to best_dermnet_model.keras\n",
      "\u001b[1m56/56\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 739ms/step - accuracy: 0.3877 - loss: 2.7313 - val_accuracy: 0.4135 - val_loss: 2.5695 - learning_rate: 2.5000e-05\n",
      "Epoch 19/25\n",
      "\u001b[1m56/56\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 694ms/step - accuracy: 0.4137 - loss: 2.7294\n",
      "Epoch 19: val_loss improved from 2.56948 to 2.56881, saving model to best_dermnet_model.keras\n",
      "\u001b[1m56/56\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 732ms/step - accuracy: 0.3984 - loss: 2.7405 - val_accuracy: 0.4202 - val_loss: 2.5688 - learning_rate: 2.5000e-05\n",
      "Epoch 20/25\n",
      "\u001b[1m56/56\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 711ms/step - accuracy: 0.3875 - loss: 2.7827\n",
      "Epoch 20: val_loss improved from 2.56881 to 2.56217, saving model to best_dermnet_model.keras\n",
      "\u001b[1m56/56\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 750ms/step - accuracy: 0.4057 - loss: 2.7235 - val_accuracy: 0.4157 - val_loss: 2.5622 - learning_rate: 2.5000e-05\n",
      "Epoch 21/25\n",
      "\u001b[1m56/56\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 676ms/step - accuracy: 0.3991 - loss: 2.7214\n",
      "Epoch 21: val_loss improved from 2.56217 to 2.54789, saving model to best_dermnet_model.keras\n",
      "\u001b[1m56/56\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m80s\u001b[0m 714ms/step - accuracy: 0.4007 - loss: 2.7277 - val_accuracy: 0.4157 - val_loss: 2.5479 - learning_rate: 2.5000e-05\n",
      "Epoch 22/25\n",
      "\u001b[1m56/56\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 712ms/step - accuracy: 0.4123 - loss: 2.6658\n",
      "Epoch 22: val_loss improved from 2.54789 to 2.52098, saving model to best_dermnet_model.keras\n",
      "\u001b[1m56/56\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 756ms/step - accuracy: 0.4063 - loss: 2.6727 - val_accuracy: 0.4360 - val_loss: 2.5210 - learning_rate: 2.5000e-05\n",
      "Epoch 23/25\n",
      "\u001b[1m56/56\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 699ms/step - accuracy: 0.4348 - loss: 2.6547\n",
      "Epoch 23: val_loss did not improve from 2.52098\n",
      "\u001b[1m56/56\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 731ms/step - accuracy: 0.4102 - loss: 2.6863 - val_accuracy: 0.4247 - val_loss: 2.5299 - learning_rate: 2.5000e-05\n",
      "Epoch 24/25\n",
      "\u001b[1m56/56\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 701ms/step - accuracy: 0.3978 - loss: 2.7421\n",
      "Epoch 24: val_loss did not improve from 2.52098\n",
      "\u001b[1m56/56\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 750ms/step - accuracy: 0.4091 - loss: 2.6994 - val_accuracy: 0.4270 - val_loss: 2.5469 - learning_rate: 2.5000e-05\n",
      "Epoch 25/25\n",
      "\u001b[1m56/56\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 724ms/step - accuracy: 0.4144 - loss: 2.6650\n",
      "Epoch 25: ReduceLROnPlateau reducing learning rate to 1.249999968422344e-05.\n",
      "\n",
      "Epoch 25: val_loss did not improve from 2.52098\n",
      "\u001b[1m56/56\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 757ms/step - accuracy: 0.4125 - loss: 2.6519 - val_accuracy: 0.4270 - val_loss: 2.5337 - learning_rate: 2.5000e-05\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(\n",
    "    datagen.flow(train_images, train_labels, batch_size=32),\n",
    "    validation_data=(val_images, val_labels),\n",
    "    epochs=25,  # reduce to save time\n",
    "    class_weight=class_weights,\n",
    "    callbacks=[early_stopping, lr_reduction, model_checkpoint],\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "68e3010b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "\u001b[1m56/56\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 764ms/step - accuracy: 0.2898 - loss: 4.6423 - val_accuracy: 0.2427 - val_loss: 5.8812\n",
      "Epoch 2/5\n",
      "\u001b[1m56/56\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 790ms/step - accuracy: 0.2763 - loss: 3.6345 - val_accuracy: 0.2494 - val_loss: 3.7224\n",
      "Epoch 3/5\n",
      "\u001b[1m56/56\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 768ms/step - accuracy: 0.2802 - loss: 3.2902 - val_accuracy: 0.2539 - val_loss: 5.8164\n",
      "Epoch 4/5\n",
      "\u001b[1m56/56\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 754ms/step - accuracy: 0.2915 - loss: 3.1711 - val_accuracy: 0.3011 - val_loss: 5.5108\n",
      "Epoch 5/5\n",
      "\u001b[1m56/56\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 767ms/step - accuracy: 0.3146 - loss: 3.1507 - val_accuracy: 0.2742 - val_loss: 4.1144\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x235aef62b50>"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for layer in model.layers[-20:]:\n",
    "    layer.trainable = True\n",
    "\n",
    "# train_labels are one-hot encoded, so use categorical_crossentropy\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "model.fit(\n",
    "    datagen.flow(train_images, train_labels, batch_size=32),\n",
    "    validation_data=(val_images, val_labels),\n",
    "    epochs=5,\n",
    "    class_weight=class_weights\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "3ec3ff74",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open(\"dermnet_training_history.json\", \"w\") as f:\n",
    "    json.dump(history.history, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "91a36df7",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_images, test_labels = load_data(r\"C:\\Users\\rrawa\\skin\\Derm\\test\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "f2cdee09",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "import pickle\n",
    "\n",
    "\n",
    "# Save model\n",
    "model.save(\"dermnet_skin_disease_model.keras\")\n",
    "# Save training history\n",
    "with open(\"dermnet_training_history.pkl\", \"wb\") as f:\n",
    "    pickle.dump(history.history, f)\n",
    "# Later when running webcam\n",
    "model = load_model(\"dermnet_skin_disease_model.keras\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "b1de5130",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1221\n",
      "Saved keys: dict_keys(['accuracy', 'loss', 'val_accuracy', 'val_loss', 'learning_rate'])\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(os.path.getsize(\"dermnet_training_history.pkl\"))  # Should be > 0\n",
    "print(\"Saved keys:\", history.history.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "59697f6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "8b1bbf2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "(918, 128, 128, 3) (918, 5)\n"
     ]
    }
   ],
   "source": [
    "print(type(test_images))\n",
    "print(type(test_labels))\n",
    "try:\n",
    "    print(test_images.shape, test_labels.shape)\n",
    "except:\n",
    "    print(\"Not numpy arrays (likely generators)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "565770e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 121ms/step - accuracy: 0.2742 - loss: 4.1144\n",
      "Test Accuracy: 0.2742\n",
      "Test Loss: 4.1144\n"
     ]
    }
   ],
   "source": [
    "# val_generator may be empty if subset/validation_split was not configured,\n",
    "# fallback to using the in-memory validation arrays (val_images, val_labels).\n",
    "try:\n",
    "\ttest_loss, test_acc = model.evaluate(val_generator, verbose=1)\n",
    "except ValueError:\n",
    "\ttest_loss, test_acc = model.evaluate(val_images, val_labels, verbose=1)\n",
    "\n",
    "print(f\"Test Accuracy: {test_acc:.4f}\")\n",
    "print(f\"Test Loss: {test_loss:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (vml)",
   "language": "python",
   "name": "vml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
